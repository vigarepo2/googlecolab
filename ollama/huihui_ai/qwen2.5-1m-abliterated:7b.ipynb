{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-ollama"
   },
   "source": [
    "# Install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-ollama-code"
   },
   "outputs": [],
   "source": [
    "# Install Ollama using curl\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start-ollama-server"
   },
   "source": [
    "# Start Ollama Server in the Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-ollama-server-code"
   },
   "outputs": [],
   "source": [
    "# Start Ollama server in the background\n",
    "!nohup ollama serve &\n",
    "\n",
    "# Give the server a few seconds to initialize\n",
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enable-t4-gpu"
   },
   "source": [
    "# Enable T4 GPU in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enable-t4-gpu-code"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pull-ollama-model"
   },
   "source": [
    "# Pull the Ollama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pull-ollama-model-code"
   },
   "outputs": [],
   "source": [
    "# Pull the model\n",
    "!ollama pull huihui_ai/qwen2.5-1m-abliterated:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-prompt-loop"
   },
   "source": [
    "# Interactive Prompt Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "interactive-prompt-loop-code"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import subprocess\n",
    "\n",
    "# Interactive prompt loop\n",
    "print(\"Interactive Ollama LLM prompt is running.\")\n",
    "print(\"Type 'exit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter your prompt: \")\n",
    "    if user_input.strip().lower() == \"exit\":\n",
    "        print(\"Exiting prompt loop.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Run the model with the user's prompt\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"huihui_ai/qwen2.5-1m-abliterated:7b\", user_input],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        # Print the model's response\n",
    "        print(\"\\nResponse:\")\n",
    "        print(result.stdout)\n",
    "    except Exception as e:\n",
    "        print(\"Error running Ollama model:\", e)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
